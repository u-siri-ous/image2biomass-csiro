{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c68bbc5",
   "metadata": {},
   "source": [
    "# ðŸŒ¾ CSIRO Image2Biomass: Data Augmentation and Training Notebook\n",
    "\n",
    "**Competition:** [CSIRO Image2Biomass Prediction](https://www.kaggle.com/competitions/csiro-biomass)\n",
    "\n",
    "**Thank you CigarCat for the useful EDA** [NB link](https://www.kaggle.com/code/takahitomizunobyts/csiro-customizable-eda)\n",
    "\n",
    "**Thank you Zhuang Jia for some of the albumentations** [NB link](https://www.kaggle.com/code/jiazhuang/csiro-simple)\n",
    "\n",
    "**Thank you Sagar Nagpure for the DINO baseline model**[NB link](https://www.kaggle.com/code/sagarnagpure1310/csiro-dinov2-tiled-lb-0-65)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ **Notebook Overview**\n",
    "\n",
    "This notebook provides **model and data preprocessing** for the CSIRO Image2Biomass competition.\n",
    "\n",
    "### **Competition Context:**\n",
    "\n",
    "**Goal:** Predict 5 biomass components from pasture images:\n",
    "- `Dry_Green_g` (10% weight)\n",
    "- `Dry_Dead_g` (10% weight)\n",
    "- `Dry_Clover_g` (10% weight)\n",
    "- `GDM_g` (20% weight)\n",
    "- `Dry_Total_g` (50% weight) â­\n",
    "\n",
    "**Evaluation:** Weighted RÂ² score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccaa0ed",
   "metadata": {},
   "source": [
    "From the EDA in the other notebook, we found out that:\n",
    "- The data is right-skewed and there are several outliers (high kurtosis)\n",
    "- Vegetation is dense, but some samples are sparser due to the data distribution\n",
    "- NDVI and height are positively correlated with biomass\n",
    "\n",
    "We need to perform data augmentation and handle outliers\n",
    "\n",
    "The primary goal is to predict five biomass targets. Based on exploratory data analysis (EDA), we identified linear dependencies:\n",
    "\n",
    "Dry_Total_g \n",
    "â‰ˆ\n",
    " Dry_Green_g + Dry_Dead_g + Dry_Clover_g\n",
    "GDM_g \n",
    "â‰ˆ\n",
    " Dry_Green_g + Dry_Clover_g\n",
    "\n",
    "To avoid redundancy, the model is trained to predict only the three most visually distinct and/or highest-weighted targets:\n",
    "\n",
    "- Dry_Total_g (50% of the score)\n",
    "- GDM_g (20% of the score)\n",
    "- Dry_Green_g (10% of the score)\n",
    "\n",
    "The remaining two targets (Dry_Dead_g and Dry_Clover_g) are then calculated during validation and inference using subtraction (e.g., pred_Clover = max(0, pred_GDM - pred_Green))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e7749",
   "metadata": {},
   "source": [
    "## 1. Setup, setting globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc7db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5adc875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CSV_PATH_TRAIN = \"./csiro-biomass/train.csv\"\n",
    "CSV_PATH_TEST = \"./csiro-biomass/test.csv\"\n",
    "IMAGE_DIR = \"./csiro-biomass\"\n",
    "\n",
    "# Target columns\n",
    "TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "\n",
    "# Competition weights\n",
    "WEIGHTS = {\n",
    "    'Dry_Green_g': 0.1,\n",
    "    'Dry_Dead_g': 0.1,\n",
    "    'Dry_Clover_g': 0.1,\n",
    "    'GDM_g': 0.2,\n",
    "    'Dry_Total_g': 0.5\n",
    "}\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Configuration set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29837a14",
   "metadata": {},
   "source": [
    "## 2. Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedad0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(csv_path: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and preprocess biomass data\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (long_format_df, wide_format_df)\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    df_long = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded {len(df_long):,} rows Ã— {len(df_long.columns)} columns\")\n",
    "    \n",
    "    # Extract image ID\n",
    "    df_long['image_id'] = df_long['sample_id'].str.split('__').str[0]\n",
    "    \n",
    "    # Convert to wide format\n",
    "    print(\"\\nðŸ”„ Converting to wide format...\")\n",
    "    meta_cols = ['image_path', 'Sampling_Date', 'State', 'Species', \n",
    "                 'Pre_GSHH_NDVI', 'Height_Ave_cm']\n",
    "    \n",
    "    df_wide = df_long.pivot_table(\n",
    "        index=['image_id'] + meta_cols,\n",
    "        columns='target_name',\n",
    "        values='target',\n",
    "        aggfunc='first'\n",
    "    ).reset_index()\n",
    "    \n",
    "    n_images = len(df_wide)\n",
    "    print(f\"Wide format: {n_images:,} images\")\n",
    "    \n",
    "    # Add temporal features\n",
    "    print(\"\\nAdding temporal features...\")\n",
    "    df_wide['date'] = pd.to_datetime(df_wide['Sampling_Date'], format='%Y/%m/%d')\n",
    "    df_wide['year'] = df_wide['date'].dt.year\n",
    "    df_wide['month'] = df_wide['date'].dt.month\n",
    "    df_wide['day'] = df_wide['date'].dt.day\n",
    "    df_wide['dayofweek'] = df_wide['date'].dt.dayofweek\n",
    "    df_wide['dayofyear'] = df_wide['date'].dt.dayofyear\n",
    "    \n",
    "    # Add season (Southern Hemisphere)\n",
    "    def get_season(month: int) -> str:\n",
    "        if month in [9, 10, 11]:\n",
    "            return \"Spring\"\n",
    "        elif month in [12, 1, 2]:\n",
    "            return \"Summer\"\n",
    "        elif month in [3, 4, 5]:\n",
    "            return \"Autumn\"\n",
    "        else:\n",
    "            return \"Winter\"\n",
    "    \n",
    "    df_wide['season'] = df_wide['month'].apply(get_season)\n",
    "    \n",
    "    # Add derived features\n",
    "    print(\"Adding derived features...\")\n",
    "    \n",
    "    # Component ratios\n",
    "    total = df_wide['Dry_Total_g'] + 1e-8 # Avoid division by 0\n",
    "    df_wide['green_ratio'] = df_wide['Dry_Green_g'] / total\n",
    "    df_wide['dead_ratio'] = df_wide['Dry_Dead_g'] / total\n",
    "    df_wide['clover_ratio'] = df_wide['Dry_Clover_g'] / total\n",
    "    \n",
    "    # Physical constraint check\n",
    "    df_wide['sum_components'] = (\n",
    "        df_wide['Dry_Green_g'] + \n",
    "        df_wide['Dry_Dead_g'] + \n",
    "        df_wide['Dry_Clover_g']\n",
    "    )\n",
    "    df_wide['constraint_diff'] = df_wide['Dry_Total_g'] - df_wide['sum_components']\n",
    "    df_wide['constraint_error'] = np.abs(df_wide['constraint_diff'])\n",
    "    \n",
    "    print(\"Preprocessing complete!\\n\")\n",
    "    \n",
    "    return df_long, df_wide\n",
    "\n",
    "# Execute\n",
    "df_long, df = load_and_preprocess_data(CSV_PATH_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b122da",
   "metadata": {},
   "source": [
    "### Transforming data to reduce skewness and outliers\n",
    "\n",
    "We apply the Yeo-Johnson transformation to make data more normally distributed and stabilize variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f6b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer \n",
    "\n",
    "numeric_features = [\n",
    "    \"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\",\n",
    "    \"Pre_GSHH_NDVI\", \"Height_Ave_cm\", \"green_ratio\", \"dead_ratio\", \"clover_ratio\",\n",
    "    \"constraint_error\"\n",
    "]\n",
    "categorical_features = [\"State\", \"Species\", \"season\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f4daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A # This works better for cpu-bound processing\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Training augmentations\n",
    "train_transforms = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    A.RandomResizedCrop(height=224, width=224, scale=(0.8, 1.0), p=1.0),\n",
    "    A.GaussianBlur(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Validation/test transforms (no augmentation, just resize + normalize)\n",
    "# TODO investigate test time augmentations!!\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize(height=256, width=256),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "class BiomassDataset(Dataset):\n",
    "    def __init__(self, df, numeric_features, categorical_features,\n",
    "                 target_cols=TARGET_COLS, train=True, scaler=None, encoder=None,\n",
    "                 transform=None):\n",
    "        self.df = df.copy()\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "\n",
    "        # Drop train-only features if not training\n",
    "        if not train:\n",
    "            numeric_features = [f for f in numeric_features if f not in [\"Pre_GSHH_NDVI\", \"Height_Ave_cm\"]]\n",
    "            categorical_features = [f for f in categorical_features if f not in [\"State\", \"Species\"]]\n",
    "\n",
    "        # Scale numeric features\n",
    "        if scaler is None:\n",
    "            self.pt = PowerTransformer(method=\"yeo-johnson\") # Data is not strictly positive :c, yeo-johnson generalizes box-cox\n",
    "            self.df[numeric_features] = self.pt.fit_transform(self.df[numeric_features])\n",
    "\n",
    "            self.scaler = RobustScaler()\n",
    "            self.df[numeric_features] = self.scaler.fit_transform(self.df[numeric_features])\n",
    "        else:\n",
    "            self.pt = scaler[0]\n",
    "            self.scaler = scaler[1]\n",
    "            self.df[numeric_features] = self.pt.transform(self.df[numeric_features])\n",
    "            self.df[numeric_features] = self.scaler.transform(self.df[numeric_features])\n",
    "\n",
    "        # Encode categorical features\n",
    "        if categorical_features:\n",
    "            if encoder is None:\n",
    "                self.encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "                encoded = self.encoder.fit_transform(self.df[categorical_features])\n",
    "            else:\n",
    "                self.encoder = encoder\n",
    "                encoded = self.encoder.transform(self.df[categorical_features])\n",
    "            encoded_df = pd.DataFrame(encoded, index=self.df.index)\n",
    "            self.features = np.hstack([self.df[numeric_features].values, encoded_df.values])\n",
    "        else:\n",
    "            self.encoder = None\n",
    "            self.features = self.df[numeric_features].values\n",
    "\n",
    "        self.targets = self.df[target_cols].values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tabular features\n",
    "        x_tab = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "\n",
    "        # Image features\n",
    "        img_path = self.df.iloc[idx][\"image_path\"]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (x_tab, image), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2315a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print tabular features dimensions\n",
    "bm = BiomassDataset(df, numeric_features, categorical_features, train=True)\n",
    "print(f\"Number of tabular features: {bm.features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ff721",
   "metadata": {},
   "source": [
    "### Using PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd33bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class BiomassDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df, numeric_features, categorical_features, target_cols=TARGET_COLS, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.numeric_features = numeric_features\n",
    "        self.categorical_features = categorical_features\n",
    "        self.target_cols = target_cols\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # TODO see if this method of splitting is okay or if stratification is needed\n",
    "        train_df, val_df = train_test_split(self.df, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "        self.train_dataset = BiomassDataset(train_df, self.numeric_features,\n",
    "                                            self.categorical_features, self.target_cols,\n",
    "                                            train=True, transform=train_transforms)\n",
    "\n",
    "        self.val_dataset = BiomassDataset(val_df, self.numeric_features,\n",
    "                                          self.categorical_features, self.target_cols,\n",
    "                                          train=False, transform=val_transforms)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31613611",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = BiomassDataModule(df, numeric_features, categorical_features)\n",
    "dm.setup()\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "print(f\"Train DataModule setup complete!\")\n",
    "val_loader = dm.val_dataloader()\n",
    "print(f\"Validation DataModule setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24740511",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of training samples: {len(dm.train_dataset)}, percentage: {len(dm.train_dataset)/len(df)*100:.2f}%\")\n",
    "print(f\"Number of validation samples: {len(dm.val_dataset)}, percentage: {len(dm.val_dataset)/len(df)*100:.2f}%\")\n",
    "\n",
    "#print tabular features dimensions\n",
    "print(f\"Number of tabular features: {dm.train_dataset.features.shape[1]}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e7036",
   "metadata": {},
   "source": [
    "## Baseline model definition\n",
    "\n",
    "Fetching a pretrained version from timm and wrapping it in Pytorch Lightning\n",
    "\n",
    "Trying to use DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e45bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "DINO_CANDIDATES = [\n",
    "    \"vit_base_patch14_dinov3\",\n",
    "    \"vit_base_patch14_reg4_dinov3\",\n",
    "    \"vit_small_patch14_dinov3\",\n",
    "    \"vit_base_patch14_reg4_dinov2\",\n",
    "    \"vit_base_patch14_dinov2\",\n",
    "    \"vit_small_patch14_dinov2\",\n",
    "]\n",
    "\n",
    "class BiomassModel(pl.LightningModule):\n",
    "    def __init__(self, tabular_input_dim, target_dim=len(TARGET_COLS), dino_model_name=\"vit_base_patch14_dinov3\", lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Load pre-trained DINO model\n",
    "        self.dino = timm.create_model(dino_model_name, pretrained=True)\n",
    "        self.dino.head = torch.nn.Identity()  # Remove classification head\n",
    "\n",
    "        dino_output_dim = self.dino.num_features\n",
    "\n",
    "        # Tabular data branch\n",
    "        self.tabular_branch = torch.nn.Sequential(\n",
    "            torch.nn.Linear(tabular_input_dim, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(64),\n",
    "        )\n",
    "\n",
    "        # Combined regression head\n",
    "        self.regression_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(dino_output_dim + 64, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(256),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(256, target_dim)\n",
    "        )\n",
    "\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x_tab, x_img):\n",
    "        # Image branch\n",
    "        img_features = self.dino(x_img)\n",
    "\n",
    "        # Tabular branch\n",
    "        tab_features = self.tabular_branch(x_tab)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat((img_features, tab_features), dim=1)\n",
    "\n",
    "        # Regression head\n",
    "        outputs = self.regression_head(combined_features)\n",
    "        return outputs\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x_tab, x_img), y = batch\n",
    "        y_pred = self(x_tab, x_img)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        (x_tab, x_img), y = batch\n",
    "        y_pred = self(x_tab, x_img)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss'\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d148bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer instance with best checkpoint saving and early stopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='./checkpoints',\n",
    "    filename=\"best-model-{epoch:02d}-{val_loss:.2f}\",  # custom filename\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "earlystopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator='auto',\n",
    "    callbacks=[checkpoint_callback, \n",
    "               earlystopping_callback,\n",
    "               ],\n",
    "    log_every_n_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "tabular_input_dim = dm.train_dataset.features.shape[1]\n",
    "model = BiomassModel(tabular_input_dim=tabular_input_dim, dino_model_name=\"vit_base_patch14_dinov3\", lr=1e-4)\n",
    "print(\"Model instantiated successfully\")\n",
    "\n",
    "# Train the model\n",
    "# TODO add cross-validation\n",
    "trainer.fit(model, dm)\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
